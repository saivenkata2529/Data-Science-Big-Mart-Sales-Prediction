# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QVCLHgrqcrBCPIUZnS3NRokmfUJ-6ACf
"""

#Big Mart Sales Prediction

"""Sales Prediction for Big Mart Outlets The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and predict the sales of each product at a particular outlet.

Using this model, BigMart will try to understand the properties of products and outlets which play a key role in increasing sales.

Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.

Data Dictionary We have train (8523) and test (5681) data set, train data set has both input and output variable(s). You need to predict the sales for test data set.

Train file: CSV containing the item outlet information with sales value Test file: CSV containing item outlet combinations for which sales need to be forecasted
"""

#Evaluation Metric

"""Evaluation Metric Your model performance will be evaluated on the basis of your prediction of the sales for the test data (test.csv), which contains similar data-points as train except for the sales to be predicted. Your submission needs to be in the format as shown in sample submission.

We at our end, have the actual sales for the test dataset, against which your predictions will be evaluated. We will use the Root Mean Square Error value to judge your response.
"""

#1Initial Setup and Data Loading

"""importing additional libraries for more sophisticated preprocessing and modeling Making a copy of the original test data is crucial - it preserves the original identifiers for the final submission Combining train and test datasets for preprocessing is a best practice because:

It ensures consistent preprocessing across both datasets It allows us to use all available data for encoding categorical variables It prevents data leakage while maintaining consistent transformations
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Load train and test datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

train_df.head()

test_df.head()

train_df.info()

train_df.describe()

train_df.shape

train_df.size

train_df.isnull().sum()

test_df.info()

test_df.describe()

test_df.shape

test_df.size

test_df.isnull().sum()

# Make a copy of the original test DataFrame for later use
original_test_df = test_df.copy()

#Feature Engineering & Data Cleaning

"""Create a New Feature (Item_Type_Combined) Item_Identifier contains product codes like "FDX20".
The first two characters (FD, NC, DR) represent Food, Non-Consumable, and Drinks.
This feature helps models differentiate between categories.
Handle Missing Values for Item_Visibility
Some Item_Visibility values are 0, which is unrealistic.
We replace 0 values with the mean visibility of that product category.
Standardize Item_Fat_Content
Item_Fat_Content has inconsistent labels (LF, low fat = Low Fat).
Standardizing labels helps avoid duplicate categories.
Handle Missing Values for Item_Weight
If Item_Weight is missing, we replace it with the average weight of similar items.
If no average exists, we fill it with the overall mean weight.
Handle Missing Values for Outlet_Size
Outlet_Size is missing in some rows.
We replace missing values with the most frequent size based on Outlet_Type.
Create Outlet_Years Feature
Item_Identifier is now redundant.
Outlet_Establishment_Year is replaced by Outlet_Years.


"""

# Feature Engineering and Data Cleaning
def preprocess_data(df):
    processed_df = df.copy()

    # Create a new feature for Item_Type_Combined
    processed_df['Item_Type_Combined'] = processed_df['Item_Identifier'].apply(lambda x: x[0:2])
    processed_df['Item_Type_Combined'] = processed_df['Item_Type_Combined'].map({
        'FD': 'Food',
        'NC': 'Non-Consumable',
        'DR': 'Drinks'
    })

    # Replace zeros in Item_Visibility with mean visibility of that product type
    visibility_avg = processed_df.pivot_table(values='Item_Visibility', index='Item_Type_Combined')
    processed_df.loc[processed_df['Item_Visibility'] == 0, 'Item_Visibility'] = \
        processed_df['Item_Type_Combined'].map(lambda x: visibility_avg.loc[x])

    # ✅ Log transformation for `Item_Visibility`
    # Convert 'Item_Visibility' to numeric before applying log1p
    processed_df['Item_Visibility'] = pd.to_numeric(processed_df['Item_Visibility'], errors='coerce')
    processed_df['Item_Visibility'] = np.log1p(processed_df['Item_Visibility'])

    # Fix Item_Fat_Content inconsistencies
    processed_df['Item_Fat_Content'] = processed_df['Item_Fat_Content'].replace({
        'LF': 'Low Fat', 'low fat': 'Low Fat', 'Low Fat': 'Low Fat',
        'reg': 'Regular', 'regular': 'Regular'
    })

    # Handle missing Item_Weight
    item_avg_weight = processed_df.pivot_table(values='Item_Weight', index='Item_Identifier')
    processed_df['Item_Weight'] = processed_df.apply(
        lambda row: item_avg_weight.loc[row['Item_Identifier']].values[0]
        if pd.isna(row['Item_Weight']) and row['Item_Identifier'] in item_avg_weight.index else row['Item_Weight'],
        axis=1
    )
    processed_df['Item_Weight'].fillna(processed_df['Item_Weight'].mean(), inplace=True)

    # Handle missing Outlet_Size
    outlet_size_mode = processed_df.pivot_table(values='Outlet_Size', columns='Outlet_Type',
                                                aggfunc=lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)
    for outlet_type, size in outlet_size_mode.items():
        processed_df.loc[(processed_df['Outlet_Type'] == outlet_type) & (processed_df['Outlet_Size'].isna()), 'Outlet_Size'] = size
    processed_df['Outlet_Size'].fillna(processed_df['Outlet_Size'].mode()[0], inplace=True)

    # ✅ Create Outlet_Age feature
    processed_df['Outlet_Years'] = 2013 - processed_df['Outlet_Establishment_Year']

    # ✅ Bin Outlet_Age into categories
    processed_df['Outlet_Age_Category'] = pd.cut(processed_df['Outlet_Years'], bins=[0, 10, 20, 30], labels=['New', 'Medium', 'Old'])

    # ✅ Create interaction features
    processed_df['Weight_Age_Interaction'] = processed_df['Item_Weight'] * processed_df['Outlet_Years']
    processed_df['Visibility_MRP_Interaction'] = processed_df['Item_Visibility'] * processed_df['Item_MRP']

    # Drop unnecessary columns
    processed_df.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)

    return processed_df

# Preprocess train and test data
train_processed = preprocess_data(train_df)
test_processed = preprocess_data(test_df)

#Encode Categorical Variables

"""Encodes categorical variables (like Outlet_Type) into numbers.
Ensures consistent encoding across train and test.
"""

# Ensure categorical encoding is consistent
def encode_categorical(train, test):
    le_dict = {}
    for column in train.select_dtypes(include=['object']).columns:
        le_dict[column] = LabelEncoder()
        combined_data = pd.concat([train[column], test[column]], axis=0)
        le_dict[column].fit(combined_data.astype(str))

        train[column] = le_dict[column].transform(train[column].astype(str))
        test[column] = test[column].map(lambda x: le_dict[column].transform([x])[0] if x in le_dict[column].classes_ else -1)

    return train, test, le_dict

# Encode categorical variables
train_encoded, test_encoded, le_dict = encode_categorical(train_processed, test_processed)

# Ensure test set has all training columns
missing_cols = set(train_encoded.columns) - set(test_encoded.columns)
for col in missing_cols:
    test_encoded[col] = 0

# Match column order
test_encoded = test_encoded[X_train.columns]

#Distribution of Target Variable (Item_Outlet_Sales)

"""A histogram is a graphical representation of the distribution of a dataset. It shows the frequency of data points falling within specific ranges (bins) along the x-axis. The y-axis represents the count or frequency of data points in each bin.

Purpose:

Understanding the Shape: The shape of the histogram tells us about the underlying distribution of the data. For example, a bell-shaped curve indicates a normal distribution, while a skewed distribution might have a longer tail on one side. Identifying Outliers: Extreme values or outliers can be identified by looking for isolated bars or gaps in the histogram. Checking for Skewness: Skewness is a measure of the asymmetry of the distribution. A histogram with a longer tail on the right side is right-skewed (positive skew), while a longer tail on the left side is left-skewed (negative skew).
"""

#Distribution of Item_Outlet_Sales (Target Variable)
plt.figure(figsize=(8, 6))
sns.histplot(train_df['Item_Outlet_Sales'], kde=True)
plt.title('Distribution of Item_Outlet_Sales')
plt.xlabel('Item_Outlet_Sales')
plt.ylabel('Frequency')
plt.show()

# Define features (X) and target variable (y)
X = train_encoded.drop('Item_Outlet_Sales', axis=1)
y = train_encoded['Item_Outlet_Sales']

#Predicted vs. Actual Sales (Validation Set)

"""A scatter plot is used to visualize the relationship between two numerical variables. Each point on the plot represents a data point, with its x-coordinate representing the value of one variable and its y-coordinate representing the value of the other variable.

Purpose:

Model Performance Assessment: By plotting predicted values against actual values, you can visually assess the model's accuracy. Ideally, the points should lie close to a diagonal line (perfect predictions). Identifying Patterns: You can look for patterns in the scatter plot, such as areas where the model tends to overpredict or underpredict. Detecting Outliers: Outliers in the predictions can be identified by points that are far away from the diagonal line.
"""

#Item_Outlet_Sales vs. Item_Visibility
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Item_Visibility', y='Item_Outlet_Sales', data=train_df)
plt.title('Item_Outlet_Sales vs. Item_Visibility')
plt.xlabel('Item_Visibility')
plt.ylabel('Item_Outlet_Sales')
plt.show()

#Item_Outlet_Sales vs. Item_MRP
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Item_MRP', y='Item_Outlet_Sales', data=train_df)
plt.title('Item_Outlet_Sales vs. Item_MRP')
plt.xlabel('Item_MRP')
plt.ylabel('Item_Outlet_Sales')
plt.show()

#Item_Outlet_Sales by Outlet_Type
plt.figure(figsize=(10, 6))
sns.boxplot(x='Outlet_Type', y='Item_Outlet_Sales', data=train_df)
plt.title('Item_Outlet_Sales by Outlet_Type')
plt.xlabel('Outlet_Type')
plt.ylabel('Item_Outlet_Sales')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

#Correlation Matrix of Numerical Features

"""A correlation matrix is a table showing the correlation coefficients between multiple variables. Each cell in the matrix represents the correlation between two variables. Correlation coefficients range from -1 to +1, where:

+1 indicates a perfect positive linear relationship (when one variable increases, the other also increases). -1 indicates a perfect negative linear relationship (when one variable increases, the other decreases). 0 indicates no linear relationship.
"""

# Correlation Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(train_encoded.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# Split data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# Define hyperparameter grid
param_distributions = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3, 0.4],
    'min_child_weight': [1, 3, 5, 7],
    'reg_alpha': [0, 0.1, 1, 10],
    'reg_lambda': [0, 0.1, 1, 10]
}

#Model Training XGBoost Model

"""Uses RandomizedSearchCV to find the best hyperparameters for XGBoost.
Re-trains XGBoost using the best parameters.
"""

# Initialize XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Increase hyperparameter tuning iterations
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_distributions,
    n_iter=50,  # Increased from 25 to 50
    scoring='neg_mean_absolute_error',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# Import OrdinalEncoder
from sklearn.preprocessing import OrdinalEncoder

# Create an OrdinalEncoder instance
encoder = OrdinalEncoder()

# Fit and transform the 'Outlet_Age_Category' column in both train and validation sets
X_train['Outlet_Age_Category'] = encoder.fit_transform(X_train[['Outlet_Age_Category']])
X_valid['Outlet_Age_Category'] = encoder.transform(X_valid[['Outlet_Age_Category']])

random_search.fit(X_train, y_train)

best_params = random_search.best_params_
best_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)
best_model.fit(X_train, y_train)

import lightgbm as lgb
# Train LightGBM with categorical_feature parameter
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
lgb_model = lgb.LGBMRegressor(random_state=42, n_estimators=300, learning_rate=0.05)
lgb_model.fit(X_train, y_train, categorical_feature=categorical_features)

import lightgbm as lgb

# ... (your existing code) ...

# Train LightGBM with categorical_feature parameter
# Get categorical features from train_encoded instead of X_train
categorical_features = train_encoded.select_dtypes(include=['object', 'category']).columns.tolist()

# Ensure these columns are also present in test_encoded and X_train, X_valid
for col in categorical_features:
    if col not in test_encoded.columns:
        test_encoded[col] = 0  # Or another suitable default value
    if col not in X_train.columns:
        X_train[col] = 0
    if col not in X_valid.columns:
        X_valid[col] = 0


# Convert categorical features to category dtype in both train and test, and X_train, X_valid
for col in categorical_features:
    train_encoded[col] = train_encoded[col].astype('category')
    test_encoded[col] = test_encoded[col].astype('category')
    X_train[col] = X_train[col].astype('category')
    X_valid[col] = X_valid[col].astype('category')


lgb_model = lgb.LGBMRegressor(random_state=42, n_estimators=300, learning_rate=0.05)
# Fit the model with X_train which has categorical features converted to category type
lgb_model.fit(X_train, y_train, categorical_feature=categorical_features)

# ... (rest of your code) ...

# Ensemble Predictions
xgb_preds = best_model.predict(X_valid)
lgb_preds = lgb_model.predict(X_valid)
final_preds = (0.6 * xgb_preds) + (0.4 * lgb_preds)

# Evaluate the ensemble model
mae_ensemble = mean_absolute_error(y_valid, final_preds)
rmse_ensemble = np.sqrt(mean_squared_error(y_valid, final_preds))

print(f'🔹 Ensemble MAE: {mae_ensemble:.2f}')
print(f'🔹 Ensemble RMSE: {rmse_ensemble:.2f}')

# Predict on test set with corrected encoding
xgb_test_preds = best_model.predict(test_encoded) # Changed best_xgb to best_model
lgb_test_preds = lgb_model.predict(test_encoded)

# Ensemble predictions
final_test_preds = (0.6 * xgb_test_preds) + (0.4 * lgb_test_preds)
final_test_preds = np.clip(final_test_preds, 0, None)

# Plot feature importance
plt.figure(figsize=(10, 8))
xgb.plot_importance(best_model, max_num_features=15, height=0.8)
plt.title('Feature Importance')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.show()

# Prepare submission
submission = pd.DataFrame({
    'Item_Identifier': original_test_df['Item_Identifier'],
    'Outlet_Identifier': original_test_df['Outlet_Identifier'],
    'Item_Outlet_Sales': final_test_preds
})

submission.to_csv('submission.csv', index=False)
print("✅ Submission file 'submission.csv' saved successfully!")
